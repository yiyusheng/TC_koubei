gbm1 <- gbm(mean~.,             # formula
data=shop_error[,c(2,9:ncol(shop_error))],                         # dataset
# var.monotone=c(0,0,0,0,0,0),    # -1: monotone decrease, +1: monotone increase,
#  0: no monotone restrictions
distribution="gaussian",        # see the help for other choices
n.trees=n_trees,                     # number of trees
shrinkage=0.1,                   # shrinkage or learning rate, 0.001 to 0.1 usually work
interaction.depth=4,             # 1: additive model, 2: two-way interactions, etc.
bag.fraction = 0.5,              # subsampling fraction, 0.5 is probably best
train.fraction = 0.5,           # fraction of data for training, first train.fraction*N used for training
n.minobsinnode = 10,             # minimum total weight needed in each node
cv.folds = 5,                     # do 3-fold cross-validation
keep.data=TRUE,                  # keep a copy of the dataset with the object
verbose=T,                   # don't print out progress
n.cores=1)                        # use only a single core (detecting #cores is error-prone, so avoided here)
# check performance using 5-fold cross-validation
best.iter <- gbm.perf(gbm1,method="cv")
gbm1 <- gbm(mean~.,             # formula
data=shop_error[,c(2,9:ncol(shop_error))],                         # dataset
# var.monotone=c(0,0,0,0,0,0),    # -1: monotone decrease, +1: monotone increase,
#  0: no monotone restrictions
distribution="gaussian",        # see the help for other choices
n.trees=n_trees,                     # number of trees
shrinkage=0.1,                   # shrinkage or learning rate, 0.001 to 0.1 usually work
interaction.depth=5,             # 1: additive model, 2: two-way interactions, etc.
bag.fraction = 0.5,              # subsampling fraction, 0.5 is probably best
train.fraction = 0.5,           # fraction of data for training, first train.fraction*N used for training
n.minobsinnode = 10,             # minimum total weight needed in each node
cv.folds = 5,                     # do 3-fold cross-validation
keep.data=TRUE,                  # keep a copy of the dataset with the object
verbose=T,                   # don't print out progress
n.cores=1)                        # use only a single core (detecting #cores is error-prone, so avoided here)
# check performance using 5-fold cross-validation
best.iter <- gbm.perf(gbm1,method="cv")
gbm1 <- gbm(mean~.,             # formula
data=shop_error[,c(2,9:ncol(shop_error))],                         # dataset
# var.monotone=c(0,0,0,0,0,0),    # -1: monotone decrease, +1: monotone increase,
#  0: no monotone restrictions
distribution="gaussian",        # see the help for other choices
n.trees=n_trees,                     # number of trees
shrinkage=0.1,                   # shrinkage or learning rate, 0.001 to 0.1 usually work
interaction.depth=10,             # 1: additive model, 2: two-way interactions, etc.
bag.fraction = 0.5,              # subsampling fraction, 0.5 is probably best
train.fraction = 0.5,           # fraction of data for training, first train.fraction*N used for training
n.minobsinnode = 10,             # minimum total weight needed in each node
cv.folds = 5,                     # do 3-fold cross-validation
keep.data=TRUE,                  # keep a copy of the dataset with the object
verbose=T,                   # don't print out progress
n.cores=1)                        # use only a single core (detecting #cores is error-prone, so avoided here)
# check performance using 5-fold cross-validation
best.iter <- gbm.perf(gbm1,method="cv")
print(best.iter)
summary(gbm1,n.trees=best.iter) # based on the estimated best number of trees
gbm1 <- gbm(mean~.,             # formula
data=shop_error[,c(2,9:ncol(shop_error))],                         # dataset
# var.monotone=c(0,0,0,0,0,0),    # -1: monotone decrease, +1: monotone increase,
#  0: no monotone restrictions
distribution="gaussian",        # see the help for other choices
n.trees=n_trees,                     # number of trees
shrinkage=0.01,                   # shrinkage or learning rate, 0.001 to 0.1 usually work
interaction.depth=10,             # 1: additive model, 2: two-way interactions, etc.
bag.fraction = 0.5,              # subsampling fraction, 0.5 is probably best
train.fraction = 0.5,           # fraction of data for training, first train.fraction*N used for training
n.minobsinnode = 10,             # minimum total weight needed in each node
cv.folds = 5,                     # do 3-fold cross-validation
keep.data=TRUE,                  # keep a copy of the dataset with the object
verbose=T,                   # don't print out progress
n.cores=1)                        # use only a single core (detecting #cores is error-prone, so avoided here)
# check performance using 5-fold cross-validation
best.iter <- gbm.perf(gbm1,method="cv")
gbm1 <- gbm(mean~.,             # formula
data=shop_error[,c(2,9:ncol(shop_error))],                         # dataset
# var.monotone=c(0,0,0,0,0,0),    # -1: monotone decrease, +1: monotone increase,
#  0: no monotone restrictions
distribution="gaussian",        # see the help for other choices
n.trees=n_trees,                     # number of trees
shrinkage=0.001,                   # shrinkage or learning rate, 0.001 to 0.1 usually work
interaction.depth=10,             # 1: additive model, 2: two-way interactions, etc.
bag.fraction = 0.5,              # subsampling fraction, 0.5 is probably best
train.fraction = 0.5,           # fraction of data for training, first train.fraction*N used for training
n.minobsinnode = 10,             # minimum total weight needed in each node
cv.folds = 5,                     # do 3-fold cross-validation
keep.data=TRUE,                  # keep a copy of the dataset with the object
verbose=T,                   # don't print out progress
n.cores=1)                        # use only a single core (detecting #cores is error-prone, so avoided here)
# check performance using 5-fold cross-validation
best.iter <- gbm.perf(gbm1,method="cv")
print(best.iter)
n_trees <- 10000
idx_error_ftr <- 2
gbm1 <- gbm(mean~.,             # formula
data=shop_error[,c(2,9:ncol(shop_error))],                         # dataset
# var.monotone=c(0,0,0,0,0,0),    # -1: monotone decrease, +1: monotone increase,
#  0: no monotone restrictions
distribution="gaussian",        # see the help for other choices
n.trees=n_trees,                     # number of trees
shrinkage=0.001,                   # shrinkage or learning rate, 0.001 to 0.1 usually work
interaction.depth=10,             # 1: additive model, 2: two-way interactions, etc.
bag.fraction = 0.5,              # subsampling fraction, 0.5 is probably best
train.fraction = 0.5,           # fraction of data for training, first train.fraction*N used for training
n.minobsinnode = 10,             # minimum total weight needed in each node
cv.folds = 5,                     # do 3-fold cross-validation
keep.data=TRUE,                  # keep a copy of the dataset with the object
verbose=T,                   # don't print out progress
n.cores=1)                        # use only a single core (detecting #cores is error-prone, so avoided here)
# check performance using 5-fold cross-validation
best.iter <- gbm.perf(gbm1,method="cv")
print(best.iter)
summary(gbm1,n.trees=best.iter) # based on the e
n_trees <- 10000
idx_error_ftr <- 2
gbm1 <- gbm(mean~.,             # formula
data=shop_error[,c(2,9:ncol(shop_error))],                         # dataset
# var.monotone=c(0,0,0,0,0,0),    # -1: monotone decrease, +1: monotone increase,
#  0: no monotone restrictions
distribution="gaussian",        # see the help for other choices
n.trees=n_trees,                     # number of trees
shrinkage=0.005,                   # shrinkage or learning rate, 0.001 to 0.1 usually work
interaction.depth=10,             # 1: additive model, 2: two-way interactions, etc.
bag.fraction = 0.5,              # subsampling fraction, 0.5 is probably best
train.fraction = 0.5,           # fraction of data for training, first train.fraction*N used for training
n.minobsinnode = 10,             # minimum total weight needed in each node
cv.folds = 5,                     # do 3-fold cross-validation
keep.data=TRUE,                  # keep a copy of the dataset with the object
verbose=T,                   # don't print out progress
n.cores=1)                        # use only a single core (detecting #cores is error-prone, so avoided here)
# check performance using 5-fold cross-validation
best.iter <- gbm.perf(gbm1,method="cv")
print(best.iter)
summary(gbm1,n.trees=best.iter) # based on the es
n_trees <- 10000
idx_error_ftr <- 4
gbm1 <- gbm(max~.,             # formula
data=shop_error[,c(idx_error_ftr,9:ncol(shop_error))],                         # dataset
# var.monotone=c(0,0,0,0,0,0),    # -1: monotone decrease, +1: monotone increase,
#  0: no monotone restrictions
distribution="gaussian",        # see the help for other choices
n.trees=n_trees,                     # number of trees
shrinkage=0.005,                   # shrinkage or learning rate, 0.001 to 0.1 usually work
interaction.depth=10,             # 1: additive model, 2: two-way interactions, etc.
bag.fraction = 0.5,              # subsampling fraction, 0.5 is probably best
train.fraction = 0.5,           # fraction of data for training, first train.fraction*N used for training
n.minobsinnode = 10,             # minimum total weight needed in each node
cv.folds = 5,                     # do 3-fold cross-validation
keep.data=TRUE,                  # keep a copy of the dataset with the object
verbose=T,                   # don't print out progress
n.cores=1)                        # use only a single core (detecting #cores is error-prone, so avoided here)
# check performance using 5-fold cross-validation
best.iter <- gbm.perf(gbm1,method="cv")
print(best.iter)
summary(gbm1,n.trees=best.iter) # based on the estimated best number of trees
summary(gbm1,n.trees=n_trees) # based on the estimated best number of trees
print(best.iter)
n_trees <- 10000
idx_error_ftr <- 5
gbm1 <- gbm(median~.,             # formula
data=shop_error[,c(idx_error_ftr,9:ncol(shop_error))],                         # dataset
# var.monotone=c(0,0,0,0,0,0),    # -1: monotone decrease, +1: monotone increase,
#  0: no monotone restrictions
distribution="gaussian",        # see the help for other choices
n.trees=n_trees,                     # number of trees
shrinkage=0.005,                   # shrinkage or learning rate, 0.001 to 0.1 usually work
interaction.depth=10,             # 1: additive model, 2: two-way interactions, etc.
bag.fraction = 0.5,              # subsampling fraction, 0.5 is probably best
train.fraction = 0.5,           # fraction of data for training, first train.fraction*N used for training
n.minobsinnode = 10,             # minimum total weight needed in each node
cv.folds = 5,                     # do 3-fold cross-validation
keep.data=TRUE,                  # keep a copy of the dataset with the object
verbose=T,                   # don't print out progress
n.cores=1)                        # use only a single core (detecting #cores is error-prone, so avoided here)
# check performance using 5-fold cross-validation
best.iter <- gbm.perf(gbm1,method="cv")
print(best.iter)
summary(gbm1,n.trees=best.iter) # based on the estimated best number of trees
n_trees <- 1000
idx_error_ftr <- 3
gbm1 <- gbm(sd~.,             # formula
data=shop_error[,c(idx_error_ftr,9:ncol(shop_error))],                         # dataset
# var.monotone=c(0,0,0,0,0,0),    # -1: monotone decrease, +1: monotone increase,
#  0: no monotone restrictions
distribution="gaussian",        # see the help for other choices
n.trees=n_trees,                     # number of trees
shrinkage=0.005,                   # shrinkage or learning rate, 0.001 to 0.1 usually work
interaction.depth=10,             # 1: additive model, 2: two-way interactions, etc.
bag.fraction = 0.5,              # subsampling fraction, 0.5 is probably best
train.fraction = 0.5,           # fraction of data for training, first train.fraction*N used for training
n.minobsinnode = 10,             # minimum total weight needed in each node
cv.folds = 5,                     # do 3-fold cross-validation
keep.data=TRUE,                  # keep a copy of the dataset with the object
verbose=T,                   # don't print out progress
n.cores=1)                        # use only a single core (detecting #cores is error-prone, so avoided here)
# check performance using 5-fold cross-validation
best.iter <- gbm.perf(gbm1,method="cv")
print(best.iter)
summary(gbm1,n.trees=best.iter) # based on the estimated best number of trees
source('D:/Git/TC_koubei/sliding_window_test.R')
View(shop_pay)
ifelse(flag_gen == 0,test_end <- as.p('2016-11-01'),test_end <- as.p('2016-11-15'))
test_start <- test_end - 14*86400
df <- subset(shop_pay,shop_id == '223')
View(df)
start_wday <- as.POSIXlt(test_start)$wday
shop_pay$wday <- as.POSIXlt(shop_pay$uni_time)$wday
df <- subset(shop_pay,shop_id == '223')
target <- subset(df,uni_time >= test_start & uni_time < test_end)
View(target)
weeks = 1
pred_start <- df$uni_time[df$wday == start_wday]
pred_end <- pred_start + 86400*7*weeks
pred_start <- df$uni_time[df$wday == start_wday & df$uni_time < test_start]
pred_end <- pred_start + 86400*7*weeks
df <- subset(shop_pay,shop_id == '223')
target <- subset(df,uni_time >= test_start & uni_time < test_end)
pred_start <- df$uni_time[df$wday == start_wday & df$uni_time < test_start]
r <- lapply(pred_start, function(ps){
pred <- subset(df,uni_time >= ps & uni_time < ps + 86400*7*weeks)
if(nrow(pred) != 7*weeks)return(-1)
else{
pred <- rep(weeks,pred$value)
return(abs(pred - target$value)/(pred + target$value))
}
})
weeks
pred$value
ps <- pred_start[1]
pred <- subset(df,uni_time >= ps & uni_time < ps + 86400*7*weeks)
ps <- pred_start[2]
pred <- subset(df,uni_time >= ps & uni_time < ps + 86400*7*weeks)
nrow(pred) != 7*weeks
pred <- rep(weeks,pred$value)
return(abs(pred - target$value)/(pred + target$value))
pred$value
weeks
rep(weeks,pred$value)
rep(1,pred$value)
rep(2,3)
pred <- rep(pred$value,weeks)
pred <- subset(df,uni_time >= ps & uni_time < ps + 86400*7*weeks)
if(nrow(pred) != 7*weeks)return(-1)
else{
pred <- rep(pred$value,2/weeks)
return(abs(pred - target$value)/(pred + target$value))
}
r <- lapply(pred_start, function(ps){
pred <- subset(df,uni_time >= ps & uni_time < ps + 86400*7*weeks)
if(nrow(pred) != 7*weeks)return(-1)
else{
pred <- rep(pred$value,2/weeks)
return(abs(pred - target$value)/(pred + target$value))
}
})
r <- lapply(pred_start, function(ps){
pred <- subset(df,uni_time >= ps & uni_time < ps + 86400*7*weeks)
if(nrow(pred) != 7*weeks)return(-1)
else{
pred <- rep(pred$value,2/weeks)
return(mean(abs(pred - target$value)/(pred + target$value)))
}
})
names(r)
r <- sapply(pred_start, function(ps){
pred <- subset(df,uni_time >= ps & uni_time < ps + 86400*7*weeks)
if(nrow(pred) != 7*weeks)return(-1)
else{
pred <- rep(pred$value,2/weeks)
return(mean(abs(pred - target$value)/(pred + target$value)))
}
})
data.frame(shop_id = df$shop_id[1],
pred_start = pred_start,
ms = r)
split_shop_pay <- split(shop_pay,shop_pay$shop_id)
source('D:/Git/TC_koubei/sliding_window_test.R')
df <- split_shop_pay[[1]]
target <- subset(df,uni_time >= test_start & uni_time < test_end)
pred_start <- df$uni_time[df$wday == start_wday & df$uni_time < test_start]
r <- sapply(pred_start, function(ps){
pred <- subset(df,uni_time >= ps & uni_time < ps + 86400*7*weeks)
if(nrow(pred) != 7*weeks)return(-1)
else{
pred <- rep(pred$value,2/weeks)
return(mean(abs(pred - target$value)/(pred + target$value)))
}
})
weeks = 1
target <- subset(df,uni_time >= test_start & uni_time < test_end)
pred_start <- df$uni_time[df$wday == start_wday & df$uni_time < test_start]
r <- sapply(pred_start, function(ps){
pred <- subset(df,uni_time >= ps & uni_time < ps + 86400*7*weeks)
if(nrow(pred) != 7*weeks)return(-1)
else{
pred <- rep(pred$value,2/weeks)
return(mean(abs(pred - target$value)/(pred + target$value)))
}
})
data.frame(shop_id = df$shop_id[1],
pred_start = pred_start,
ms = r)
sliding_window <- function(df,weeks = 1){
print(df$shop_id[1])
target <- subset(df,uni_time >= test_start & uni_time < test_end)
pred_start <- df$uni_time[df$wday == start_wday & df$uni_time < test_start]
r <- sapply(pred_start, function(ps){
pred <- subset(df,uni_time >= ps & uni_time < ps + 86400*7*weeks)
if(nrow(pred) != 7*weeks)return(-1)
else{
pred <- rep(pred$value,2/weeks)
return(mean(abs(pred - target$value)/(pred + target$value)))
}
})
data.frame(shop_id = df$shop_id[1],
pred_start = pred_start,
ms = r)
}
r <- lapply(split_shop_pay,sliding_window,weeks = 1)
warnings()
sliding_window <- function(df,weeks = 1){
target <- subset(df,uni_time >= test_start & uni_time < test_end)
if(nrow(target) != 14)return(data.frame(shop_id = df$shop_id[1],
pred_start = target$uni_time[1],
ms = -2))
pred_start <- df$uni_time[df$wday == start_wday & df$uni_time < test_start]
r <- sapply(pred_start, function(ps){
pred <- subset(df,uni_time >= ps & uni_time < ps + 86400*7*weeks)
if(nrow(pred) != 7*weeks)return(-1)
else{
pred <- rep(pred$value,2/weeks)
return(mean(abs(pred - target$value)/(pred + target$value)))
}
})
data.frame(shop_id = df$shop_id[1],
pred_start = pred_start,
ms = r)
}
r <- lapply(split_shop_pay,sliding_window,weeks = 1)
r7 <- r
r7 <- do.call(rbind,r)
View(r7)
a <- subset(r7,ms == -2)
View(a)
df <- subset(r7,shop_id == 1)
View(df)
df <- df[order(df$ms),]
View(r7)
df <- subset(r7,shop_id == 1,ms >= 0)
df <- df[order(df$ms),]
View(df)
df <- subset(r7,shop_id == 1 & ms >= 0)
df <- df[order(df$ms),]
df <- df[order(df$ms),]
df$id <- 1:nrow(df)
View(df)
dcast(shop_id~id,data = df[,c('shop_id','id','ms')],value.var = 'id')
df[,c('shop_id','id','ms')]
dcast(shop_id~id,data = df[,c('shop_id','id','ms')],value.var = 'ms')
dcast(shop_id~id,data = df[1:10,c('shop_id','id','ms')],value.var = 'ms')
sta_r7 <- by(r7,r7$shop_id,function(df){
df <- subset(r7,shop_id == 1 & ms >= 0)
if(nrow(df) == 0)return(NULL)
df <- df[order(df$ms),]
df$id <- 1:nrow(df)
return(list(ms = dcast(shop_id~id,data = df[1:10,c('shop_id','id','ms')],value.var = 'ms'),
date = dcast(shop_id~id,data = df[1:10,c('shop_id','id','pred_start')],value.var = 'pred_start')))
})
sta_r7 <- do.call(rbind,sta_r7)
View(sta_r7)
sta_r7 <- by(r7,r7$shop_id,function(df){
df <- subset(r7,shop_id == 1 & ms >= 0)
if(nrow(df) == 0)return(NULL)
df <- df[order(df$ms),]
df$id <- 1:nrow(df)
return(list(ms = dcast(shop_id~id,data = df[1:10,c('shop_id','id','ms')],value.var = 'ms'),
date = dcast(shop_id~id,data = df[1:10,c('shop_id','id','pred_start')],value.var = 'pred_start')))
})
sta_r7_ms <- lapply(sta_r7,'[[','ms')
sta_r7_ms <- lapply(sta_r7,'[[','ms');sta_r7_ms <- do.call(rbind,sta_r7_ms)
View(sta_r7_ms)
sta_r7 <- by(r7,r7$shop_id,function(df){
if(nrow(df) == 0)return(NULL)
df <- df[order(df$ms),]
df$id <- 1:nrow(df)
return(list(ms = dcast(shop_id~id,data = df[1:10,c('shop_id','id','ms')],value.var = 'ms'),
date = dcast(shop_id~id,data = df[1:10,c('shop_id','id','pred_start')],value.var = 'pred_start')))
})
sta_r7_ms <- lapply(sta_r7,'[[','ms');sta_r7_ms <- do.call(rbind,sta_r7_ms)
a <- subset(r7,shop_id == 2)
View(a)
sta_r7 <- by(r7,r7$shop_id,function(df){
if(nrow(df) == 1)return(NULL)
df <- df[order(df$ms),]
df$id <- 1:nrow(df)
return(list(ms = dcast(shop_id~id,data = df[1:10,c('shop_id','id','ms')],value.var = 'ms'),
date = dcast(shop_id~id,data = df[1:10,c('shop_id','id','pred_start')],value.var = 'pred_start')))
})
sta_r7_ms <- lapply(sta_r7,'[[','ms');sta_r7_ms <- do.call(rbind,sta_r7_ms)
View(sta_r7_ms)
sta_r7 <- by(r7,r7$shop_id,function(df){
if(nrow(df) == 1)return(NULL)
df <- subset(df,ms >= 0)
df <- df[order(df$ms),]
df$id <- 1:nrow(df)
return(list(ms = dcast(shop_id~id,data = df[1:10,c('shop_id','id','ms')],value.var = 'ms'),
date = dcast(shop_id~id,data = df[1:10,c('shop_id','id','pred_start')],value.var = 'pred_start')))
})
sta_r7_ms <- lapply(sta_r7,'[[','ms');sta_r7_ms <- do.call(rbind,sta_r7_ms)
df <- subset(r7,shop_id == 1)
sta_r7 <- by(r7,r7$shop_id,function(df){
df <- subset(df,ms >= 0)
if(nrow(df) == 0)return(NULL)
df <- df[order(df$ms),]
df$id <- 1:nrow(df)
return(list(ms = dcast(shop_id~id,data = df[1:10,c('shop_id','id','ms')],value.var = 'ms'),
date = dcast(shop_id~id,data = df[1:10,c('shop_id','id','pred_start')],value.var = 'pred_start')))
})
sta_r7_ms <- lapply(sta_r7,'[[','ms');sta_r7_ms <- do.call(rbind,sta_r7_ms)
sta_r7 <- by(r7,r7$shop_id,function(df){
if(nrow(df) == 1)return(NULL)
df <- df[order(df$ms),]
df$id <- 1:nrow(df)
return(list(ms = dcast(shop_id~id,data = df[1:10,c('shop_id','id','ms')],value.var = 'ms'),
date = dcast(shop_id~id,data = df[1:10,c('shop_id','id','pred_start')],value.var = 'pred_start')))
})
sta_r7_ms <- lapply(sta_r7,'[[','ms');sta_r7_ms <- do.call(rbind,sta_r7_ms)
View(sta_r7_ms)
sta_r7 <- by(r7,r7$shop_id,function(df){
if(nrow(df) == 1)return(NULL)
df <- df[order(df$ms),]
df <- subset(df,ms >= 0)
df$id <- 1:nrow(df)
return(list(ms = dcast(shop_id~id,data = df[1:10,c('shop_id','id','ms')],value.var = 'ms'),
date = dcast(shop_id~id,data = df[1:10,c('shop_id','id','pred_start')],value.var = 'pred_start')))
})
sta_r7 <- by(r7,r7$shop_id,function(df){
if(nrow(df) == 1)return(NULL)
df <- df[order(df$ms),]
df <- subset(df,ms >= 0)
df$id <- 1:nrow(df)
return(list(ms = dcast(shop_id~id,data = df[1:5,c('shop_id','id','ms')],value.var = 'ms'),
date = dcast(shop_id~id,data = df[1:5,c('shop_id','id','pred_start')],value.var = 'pred_start')))
})
sta_r7_ms <- lapply(sta_r7,'[[','ms');sta_r7_ms <- do.call(rbind,sta_r7_ms)
View(sta_r7_ms)
colMeans(sta_r7_ms)
source('D:/Git/TC_koubei/sliding_window_test.R')
rm(list = ls())
source('head.R')
source('gen_result_last_week_Func.R')
load(file.path(dir_data,'shop_pay.Rda'))
load(file.path(dir_data,'shop_info.Rda'))
flag_gen <- 0
ifelse(flag_gen == 0,test_end <- as.p('2016-11-01'),test_end <- as.p('2016-11-15'))
test_start <- test_end - 14*86400
start_wday <- as.POSIXlt(test_start)$wday
shop_pay$wday <- as.POSIXlt(shop_pay$uni_time)$wday
split_shop_pay <- split(shop_pay,shop_pay$shop_id)
sliding_window <- function(df,win_size = 7){
target <- subset(df,uni_time >= test_start & uni_time < test_end)
if(nrow(target) != 14)return(data.frame(shop_id = df$shop_id[1],
pred_start = target$uni_time[1],
ms = -2))
pred_start <- df$uni_time[df$wday == start_wday & df$uni_time < test_start]
r <- sapply(pred_start, function(ps){
pred <- subset(df,uni_time >= ps & uni_time < ps + 86400*win_size)
if(nrow(pred) != win_size)return(-1)
else{
pred <- rep(pred$value,win_size/7)
return(mean(abs(pred - target$value)/(pred + target$value)))
}
})
data.frame(shop_id = df$shop_id[1],
pred_start = pred_start,
ms = r)
}
r7 <- lapply(split_shop_pay,sliding_window,win_size = 7)
r7 <- do.call(rbind,r7)
r14 <- lapply(split_shop_pay,sliding_window,win_size = 14)
r14 <- do.call(rbind,r14)
sta_r7 <- by(r7,r7$shop_id,function(df){
if(nrow(df) == 1)return(NULL)
df <- df[order(df$ms),]
df <- subset(df,ms >= 0)
df$id <- 1:nrow(df)
return(list(ms = dcast(shop_id~id,data = df[1:5,c('shop_id','id','ms')],value.var = 'ms'),
date = dcast(shop_id~id,data = df[1:5,c('shop_id','id','pred_start')],value.var = 'pred_start')))
})
sta_r7_ms <- lapply(sta_r7,'[[','ms');sta_r7_ms <- do.call(rbind,sta_r7_ms)
sta_r7_date <- lapply(sta_r7,'[[','date');sta_r7_date <- do.call(rbind,sta_r7_date)
sta_r14 <- by(r14,r14$shop_id,function(df){
if(nrow(df) == 1)return(NULL)
df <- df[order(df$ms),]
df <- subset(df,ms >= 0)
df$id <- 1:nrow(df)
return(list(ms = dcast(shop_id~id,data = df[1:2,c('shop_id','id','ms')],value.var = 'ms'),
date = dcast(shop_id~id,data = df[1:2,c('shop_id','id','pred_start')],value.var = 'pred_start')))
})
sta_r14_ms <- lapply(sta_r14,'[[','ms');sta_r14_ms <- do.call(rbind,sta_r14_ms)
sta_r14_date <- lapply(sta_r14,'[[','date');sta_r14_date <- do.call(rbind,sta_r14_date)
colMeans(sta_r7_ms)
colMeans(sta_r14_ms)
View(sta_r7_date)
sta_r7 <- by(r7,r7$shop_id,function(df){
if(nrow(df) == 1)return(NULL)
df <- df[order(df$ms),]
df <- subset(df,ms >= 0)
df$id <- 1:nrow(df)
df$pred_start <- as.character(df$pred_start)
return(list(ms = dcast(shop_id~id,data = df[1:5,c('shop_id','id','ms')],value.var = 'ms'),
date = dcast(shop_id~id,data = df[1:5,c('shop_id','id','pred_start')],value.var = 'pred_start')))
})
sta_r7_ms <- lapply(sta_r7,'[[','ms');sta_r7_ms <- do.call(rbind,sta_r7_ms)
sta_r7_date <- lapply(sta_r7,'[[','date');sta_r7_date <- do.call(rbind,sta_r7_date)
View(sta_r7_date)
